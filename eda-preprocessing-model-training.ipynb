{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- create configuration ---\n\nclass Config:\n    target = 'diagnosed_diabetes'\n    train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv', index_col='id')\n    test = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv', index_col='id')\n    submission = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n    orig = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n    train = pd.concat([train, orig[train.columns]], axis=0, ignore_index=True)\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    state = 42\n    n_splits = 10\n    early_stop = 200\n    metric = 'roc_auc'\n    task_type = \"binary\"\n    task_is_regression = task_type == 'regression'\n    if task_is_regression:\n        n_classes = None\n    else:\n        n_classes = train[target].nunique()\n        labels = list(train[target].unique())\n\n    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=state)\n\n    outliers = False\n    log_trf = False\n    missing = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- EDA ---\n## --- Version 1 ---\n\nclass EDA(Config):\n    \n    def __init__(self):\n        super().__init__()\n\n        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool']).columns.tolist()\n        self.data_info()\n        self.heatmap()\n        self.dist_plots()\n        self.cat_feature_plots()\n        if self.task_is_regression:\n            self.target_plot()\n        else:\n            self.target_pie()\n                \n    def data_info(self):\n        \n        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n            table_style = [{'selector': 'th:not(.index_name)',\n                            'props': [('background-color', '#3cb371'),\n                                      ('color', '#FFFFFF'),\n                                      ('font-weight', 'bold'),\n                                      ('border', '1px solid #DCDCDC'),\n                                      ('text-align', 'center')]\n                            }, \n                            {'selector': 'tbody td',\n                             'props': [('border', '1px solid #DCDCDC'),\n                                       ('font-weight', 'normal')]\n                            }]\n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} head\\n')\n            display(data.head().style.set_table_styles(table_style))\n                           \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} info\\n'+Style.RESET_ALL)               \n            display(data.info())\n                           \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} describe\\n')\n            display(data.describe().drop(index='count', columns=self.target, errors = 'ignore').T\n                    .style.set_table_styles(table_style).format('{:.3f}'))\n            \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} missing values\\n'+Style.RESET_ALL)               \n            display(data.isna().sum())\n        return self\n    \n    def heatmap(self):\n        print(Style.BRIGHT+Fore.GREEN+f'\\nCorrelation Heatmap\\n')\n        plt.figure(figsize=(10, 10))\n        corr = self.train[self.num_features+[self.target]].corr(method='pearson')\n        sns.heatmap(corr, fmt = '0.2f', cmap = 'Greens', square=True, annot=True, linewidths=1, cbar=False)\n        plt.show()\n        \n    def dist_plots(self):\n        print(Style.BRIGHT+Fore.GREEN+f\"\\nDistribution analysis\\n\")\n        df = pd.concat([self.train[self.num_features].assign(Source = 'Train'), \n                        self.test[self.num_features].assign(Source = 'Test'),], \n                        axis=0, ignore_index = True)\n\n        fig, axes = plt.subplots(len(self.num_features), 2 ,figsize = (18, len(self.num_features) * 6), \n                                 gridspec_kw = {'hspace': 0.3, \n                                                'wspace': 0.2, \n                                                'width_ratios': [0.70, 0.30]\n                                               }\n                                )\n        for i,col in enumerate(self.num_features):\n            ax = axes[i,0]\n            sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source', \n                        palette = ['#3cb371', 'r'], ax = ax, linewidth = 2\n                       )\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col}\")\n            ax.grid()\n\n            ax = axes[i,1]\n            sns.boxplot(data = df, y = col, x=df.Source, width = 0.5,\n                        linewidth = 1, fliersize= 1,\n                        ax = ax, palette=['#3cb371', 'r']\n                       )\n            ax.set_title(f\"\\n{col}\")\n            ax.set(xlabel = '', ylabel = '')\n            ax.tick_params(axis='both', which='major')\n            ax.set_xticklabels(['Train', 'Test'])\n\n        plt.tight_layout()\n        plt.show()\n               \n    def cat_feature_plots(self):\n        fig, axes = plt.subplots(max(len(self.cat_features), 1), 2 ,figsize = (18, len(self.cat_features) * 6), \n                                 gridspec_kw = {'hspace': 0.5, \n                                                'wspace': 0.2,\n                                               }\n                                )\n        if len(self.cat_features) == 1:\n            axes = np.array([axes])\n            \n        for i, col in enumerate(self.cat_features):\n            ax = axes[i,0]\n            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='#3cb371')\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col} Train\")\n            \n            ax = axes[i,1]\n            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='r')\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col} Test\")\n\n        plt.tight_layout()\n        plt.show()\n\n    def target_pie(self):\n        print(Style.BRIGHT+Fore.GREEN+f\"\\nTarget feature distribution\\n\")\n        targets = self.train[self.target]\n        plt.figure(figsize=(6, 6))\n        plt.pie(targets.value_counts(), labels=targets.value_counts().index, autopct='%1.2f%%', colors=sns.color_palette('viridis', len(targets.value_counts())))\n        plt.show()\n\n    def target_plot(self):\n        print(Style.BRIGHT+Fore.GREEN+f\"\\nTarget feature distribution\\n\")\n        \n        fig, axes = plt.subplots(1, 2 ,figsize = (14, 6), \n                                 gridspec_kw = {'hspace': 0.3, \n                                                'wspace': 0.2, \n                                                'width_ratios': [0.70, 0.30]\n                                               }\n                                )\n        ax = axes[0]\n        sns.kdeplot(data = self.train[self.target], \n                    color = '#3cb371', ax = ax, linewidth = 2\n                   )\n        ax.set(xlabel = '', ylabel = '')\n        ax.set_title(f\"\\n{self.target}\")\n        ax.grid()\n\n        ax = axes[1]\n        sns.boxplot(data = self.train, y = self.target, width = 0.5,\n                    linewidth = 1, fliersize= 1,\n                    ax = ax, color = '#3cb371'\n                   )\n        ax.set_title(f\"\\n{self.target}\")\n        ax.set(xlabel = '', ylabel = '')\n        ax.tick_params(axis='both', which='major')\n\n        plt.tight_layout()\n        plt.show()\n\n\n\n## --- Version 2 ---\n\nclass EDA(Config, Preprocessing):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.data_info()\n        self.heatmap()\n        self.dist_plots()\n        self.cat_feature_plots()\n        self.target_pie()\n                \n    def data_info(self):\n        \n        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n            table_style = [{'selector': 'th:not(.index_name)',\n                            'props': [('background-color', '#3cb371'),\n                                      ('color', '#FFFFFF'),\n                                      ('font-weight', 'bold'),\n                                      ('border', '1px solid #DCDCDC'),\n                                      ('text-align', 'center')]\n                            }, \n                            {'selector': 'tbody td',\n                             'props': [('border', '1px solid #DCDCDC'),\n                                       ('font-weight', 'normal')]\n                            }]\n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} head\\n')\n            display(data.head().style.set_table_styles(table_style))\n                           \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} info\\n'+Style.RESET_ALL)               \n            display(data.info())\n                           \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} describe\\n')\n            display(data.describe().drop(index='count', columns=self.targets, errors = 'ignore').T\n                    .style.set_table_styles(table_style).format('{:.3f}'))\n            \n            print(Style.BRIGHT+Fore.GREEN+f'\\n{label} missing values\\n'+Style.RESET_ALL)               \n            display(data.isna().sum())\n        return self\n    \n    def heatmap(self):\n        print(Style.BRIGHT+Fore.GREEN+f'\\nCorrelation Heatmap\\n')\n        plt.figure(figsize=(7,7))\n        corr = self.train.select_dtypes(exclude='object').corr(method='pearson')\n        sns.heatmap(corr, fmt = '0.2f', cmap = 'Greens', annot=True, cbar=False)\n        plt.show()\n        \n    def dist_plots(self):\n        print(Style.BRIGHT+Fore.GREEN+f\"\\nDistribution analysis\\n\")\n        df = pd.concat([self.train[self.num_features].assign(Source = 'Train'), \n                        self.test[self.num_features].assign(Source = 'Test'),], \n                        axis=0, ignore_index = True)\n\n        fig, axes = plt.subplots(len(self.num_features), 2 ,figsize = (18, len(self.num_features) * 6), \n                                 gridspec_kw = {'hspace': 0.3, \n                                                'wspace': 0.2, \n                                                'width_ratios': [0.70, 0.30]\n                                               }\n                                )\n        for i,col in enumerate(self.num_features):\n            ax = axes[i,0]\n            sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source', \n                        palette = ['#3cb371', 'r'], ax = ax, linewidth = 2\n                       )\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col}\")\n            ax.grid()\n\n            ax = axes[i,1]\n            sns.boxplot(data = df, y = col, x=df.Source, width = 0.5,\n                        linewidth = 1, fliersize= 1,\n                        ax = ax, palette=['#3cb371', 'r']\n                       )\n            ax.set_title(f\"\\n{col}\")\n            ax.set(xlabel = '', ylabel = '')\n            ax.tick_params(axis='both', which='major')\n            ax.set_xticklabels(['Train', 'Test'])\n\n        plt.tight_layout()\n        plt.show()\n               \n    def cat_feature_plots(self):\n        fig, axes = plt.subplots(len(self.cat_features), 2 ,figsize = (18, len(self.cat_features) * 6), \n                                 gridspec_kw = {'hspace': 0.5, \n                                                'wspace': 0.2,\n                                               }\n                                )\n\n        for i, col in enumerate(self.cat_features):\n            \n            ax = axes[i,0]\n            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='#3cb371')\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col} Train\")\n            \n            ax = axes[i,1]\n            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='r')\n            ax.set(xlabel = '', ylabel = '')\n            ax.set_title(f\"\\n{col} Test\")\n\n        plt.tight_layout()\n        plt.show()\n        \n    def target_pie(self):\n        print(Style.BRIGHT+Fore.GREEN+f\"\\nTarget feature distribution\\n\")\n        targets = self.train[self.targets]\n        plt.figure(figsize=(6, 6))\n        plt.pie(targets.value_counts(), labels=targets.value_counts().index, autopct='%1.2f%%', colors=sns.color_palette('viridis', len(targets.value_counts())))\n        plt.show()   \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Version 3 ---\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom colorama import Style, Fore, init\nfrom IPython.display import display\n\ninit(autoreset=True)\n\n\nclass EDA(Config, Preprocessing):\n    \n\n    def __init__(self, train=None, test=None, target=None, targets=None, task_is_regression=None):\n        super().__init__()\n\n        # Allow overriding attributes passed to ctor; otherwise rely on base classes\n        if train is not None:\n            self.train = train\n        if test is not None:\n            self.test = test\n        if target is not None:\n            self.target = target\n        if targets is not None:\n            self.targets = targets\n        if task_is_regression is not None:\n            self.task_is_regression = task_is_regression\n\n        # Validate presence\n        if not hasattr(self, 'train') or self.train is None:\n            raise ValueError(\"`train` DataFrame is required (either pass it or provide in Config).\")\n        if not hasattr(self, 'test') or self.test is None:\n            # test is optional, but set to empty frame for safe concat\n            self.test = pd.DataFrame(columns=self.train.columns)\n\n        # Normalize targets: prefer `targets` (list-like), fallback to `target` (single)\n        if hasattr(self, 'targets') and self.targets:\n            self.target_list = list(self.targets) if isinstance(self.targets, (list, tuple)) else [self.targets]\n        elif hasattr(self, 'target') and self.target:\n            self.target_list = [self.target]\n        else:\n            self.target_list = []\n\n        # Determine categorical and numerical features excluding target columns\n        drop_cols = self.target_list if self.target_list else []\n        self.cat_features = self.train.drop(columns=drop_cols, errors='ignore') \\\n                                     .select_dtypes(include=['object', 'bool', 'category']).columns.tolist()\n        self.num_features = self.train.drop(columns=drop_cols, errors='ignore') \\\n                                     .select_dtypes(include=[np.number]).columns.tolist()\n\n        # Run EDA steps\n        self.data_info()\n        self.heatmap()\n        self.dist_plots()\n        self.cat_feature_plots()\n        # Target visualizations (handle multi-target)\n        if len(self.target_list) == 0:\n            print(Style.BRIGHT + Fore.YELLOW + \"\\nNo target provided; skipping target plots.\")\n        else:\n            for t in self.target_list:\n                if getattr(self, 'task_is_regression', None):\n                    self.target_plot(t)\n                else:\n                    self.target_pie(t)\n\n    def data_info(self):\n        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n            table_style = [\n                {'selector': 'th:not(.index_name)',\n                 'props': [('background-color', '#3cb371'),\n                           ('color', '#FFFFFF'),\n                           ('font-weight', 'bold'),\n                           ('border', '1px solid #DCDCDC'),\n                           ('text-align', 'center')]},\n                {'selector': 'tbody td',\n                 'props': [('border', '1px solid #DCDCDC'),\n                           ('font-weight', 'normal')]}\n            ]\n\n            print(Style.BRIGHT + Fore.GREEN + f'\\n{label} head\\n')\n            display(data.head().style.set_table_styles(table_style))\n\n            print(Style.BRIGHT + Fore.GREEN + f'\\n{label} info\\n' + Style.RESET_ALL)\n            display(data.info())\n\n            print(Style.BRIGHT + Fore.GREEN + f'\\n{label} describe\\n')\n            # drop counts row and target columns from describe view for clarity\n            desc = data.describe().drop(index=['count'], errors='ignore')\n            if self.target_list:\n                desc = desc.drop(columns=self.target_list, errors='ignore')\n            display(desc.T.style.set_table_styles(table_style).format('{:.3f}'))\n\n            print(Style.BRIGHT + Fore.GREEN + f'\\n{label} missing values\\n' + Style.RESET_ALL)\n            display(data.isna().sum())\n\n        return self\n\n    def heatmap(self):\n        print(Style.BRIGHT + Fore.GREEN + f'\\nCorrelation Heatmap\\n')\n\n        # Select numeric columns (including numeric targets if present)\n        numeric_cols = self.train.select_dtypes(include=[np.number]).columns.tolist()\n        if not numeric_cols:\n            print(Style.BRIGHT + Fore.YELLOW + \"No numeric columns available for correlation heatmap.\")\n            return self\n\n        corr = self.train[numeric_cols].corr(method='pearson')\n\n        plt.figure(figsize=(8, 8))\n        sns.heatmap(corr, fmt='.2f', cmap='Greens', annot=True, cbar=False, square=True, linewidths=0.5)\n        plt.title('Correlation Heatmap')\n        plt.show()\n        return self\n\n    def dist_plots(self):\n        print(Style.BRIGHT + Fore.GREEN + f\"\\nDistribution analysis\\n\")\n\n        if len(self.num_features) == 0:\n            print(Style.BRIGHT + Fore.YELLOW + \"No numerical features to plot distributions for.\")\n            return self\n\n        # Prepare combined df with Source marker; ensure columns exist in both frames\n        train_num = self.train[self.num_features].copy()\n        train_num['Source'] = 'Train'\n        test_num = self.test.reindex(columns=self.num_features).copy()\n        test_num['Source'] = 'Test'\n        df = pd.concat([train_num, test_num], axis=0, ignore_index=True)\n\n        n = len(self.num_features)\n        fig, axes = plt.subplots(n, 2, figsize=(18, max(6 * n, 6)),\n                                 gridspec_kw={'hspace': 0.3, 'wspace': 0.2, 'width_ratios': [0.7, 0.3]})\n\n        # Normalise axes shape to (n,2)\n        if n == 1:\n            axes = np.atleast_2d(axes)\n\n        for i, col in enumerate(self.num_features):\n            ax_kde = axes[i, 0]\n            ax_box = axes[i, 1]\n\n            try:\n                sns.kdeplot(data=df, x=col, hue='Source', palette=['#3cb371', 'r'], ax=ax_kde, linewidth=2)\n            except Exception:\n                # fallback to hist if kde fails (e.g., many identical values)\n                sns.histplot(data=df, x=col, hue='Source', palette=['#3cb371', 'r'], ax=ax_kde, element='step', stat='density')\n\n            ax_kde.set(xlabel='', ylabel='')\n            ax_kde.set_title(f\"\\n{col}\")\n            ax_kde.grid()\n\n            # Boxplot for Train vs Test\n            sns.boxplot(data=df, y=col, x='Source', width=0.5, linewidth=1, fliersize=1, ax=ax_box,\n                        palette=['#3cb371', 'r'])\n            ax_box.set_title(f\"\\n{col}\")\n            ax_box.set(xlabel='', ylabel='')\n            ax_box.tick_params(axis='both', which='major')\n            ax_box.set_xticklabels(['Train', 'Test'])\n\n        plt.tight_layout()\n        plt.show()\n        return self\n\n    def cat_feature_plots(self):\n        if len(self.cat_features) == 0:\n            print(Style.BRIGHT + Fore.YELLOW + \"No categorical features to plot.\")\n            return self\n\n        n = len(self.cat_features)\n        fig, axes = plt.subplots(n, 2, figsize=(18, max(6 * n, 6)),\n                                 gridspec_kw={'hspace': 0.5, 'wspace': 0.2})\n        if n == 1:\n            axes = np.atleast_2d(axes)\n\n        for i, col in enumerate(self.cat_features):\n            # Prepare top-k categories for stable plotting\n            train_counts = self.train[col].value_counts().nlargest(10).reset_index()\n            train_counts.columns = [col, 'count']\n            test_counts = self.test[col].value_counts().nlargest(10).reset_index()\n            test_counts.columns = [col, 'count']\n\n            ax_train = axes[i, 0]\n            sns.barplot(data=train_counts, x=col, y='count', ax=ax_train, color='#3cb371')\n            ax_train.set(xlabel='', ylabel='')\n            ax_train.set_title(f\"\\n{col} Train\")\n            ax_train.tick_params(axis='x', rotation=45)\n\n            ax_test = axes[i, 1]\n            sns.barplot(data=test_counts, x=col, y='count', ax=ax_test, color='r')\n            ax_test.set(xlabel='', ylabel='')\n            ax_test.set_title(f\"\\n{col} Test\")\n            ax_test.tick_params(axis='x', rotation=45)\n\n        plt.tight_layout()\n        plt.show()\n        return self\n\n    def target_pie(self, target_col):\n        print(Style.BRIGHT + Fore.GREEN + f\"\\nTarget `{target_col}` distribution (pie)\\n\")\n        if target_col not in self.train.columns:\n            print(Style.BRIGHT + Fore.YELLOW + f\"Target `{target_col}` not found in train data.\")\n            return self\n\n        values = self.train[target_col].value_counts()\n        plt.figure(figsize=(6, 6))\n        plt.pie(values, labels=values.index.astype(str), autopct='%1.2f%%',\n                colors=sns.color_palette('viridis', len(values)))\n        plt.title(f\"{target_col} distribution\")\n        plt.show()\n        return self\n\n    def target_plot(self, target_col):\n        print(Style.BRIGHT + Fore.GREEN + f\"\\nTarget `{target_col}` distribution (regression)\\n\")\n        if target_col not in self.train.columns:\n            print(Style.BRIGHT + Fore.YELLOW + f\"Target `{target_col}` not found in train data.\")\n            return self\n\n        fig, axes = plt.subplots(1, 2, figsize=(14, 6), gridspec_kw={'width_ratios': [0.7, 0.3], 'wspace': 0.2})\n        ax_kde = axes[0]\n        ax_box = axes[1]\n\n        try:\n            sns.kdeplot(data=self.train, x=target_col, color='#3cb371', ax=ax_kde, linewidth=2)\n        except Exception:\n            sns.histplot(data=self.train, x=target_col, color='#3cb371', ax=ax_kde, stat='density', element='step')\n\n        ax_kde.set(xlabel='', ylabel='')\n        ax_kde.set_title(f\"\\n{target_col}\")\n        ax_kde.grid()\n\n        sns.boxplot(data=self.train, y=target_col, width=0.5, linewidth=1, fliersize=1, ax=ax_box, color='#3cb371')\n        ax_box.set_title(f\"\\n{target_col}\")\n        ax_box.set(xlabel='', ylabel='')\n        ax_box.tick_params(axis='both', which='major')\n\n        plt.tight_layout()\n        plt.show()\n        return self","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Preprocessing ---\n\n## --- Version 1 ---\n\nclass Preprocessing(Config):\n    \n    def __init__(self, n_splits=5, random_state=42, smoothing=20):\n        super().__init__()\n        self.global_stats = {}\n        self.encodings = {}\n        self.freq_encodings = {}\n        self.count_encodings = {}\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.smoothing = smoothing\n\n    def fit_transform(self):\n        self.prepare_data()\n        if self.missing:\n            self.missing_values()\n\n        combine = pd.concat([self.X, self.test])\n        combine = self.feature_engineering(combine)\n        self.X = combine.iloc[:len(self.X)].copy()\n        self.test = combine.iloc[len(self.X):].copy()\n\n        self.num_features = self.test.select_dtypes(exclude=['object', 'bool', 'category']).columns.tolist()\n        self.cat_features = self.test.select_dtypes(include=['object', 'bool','category']).columns.tolist()\n\n        if self.outliers:\n            self.remove_outliers()\n        if self.log_trf:\n            self.log_transformation()\n\n        return self.X, self.y, self.test, self.cat_features, self.num_features\n\n    def prepare_data(self):\n        self.train_raw = self.train.copy()\n        self.y = self.train[self.target]\n        self.X = self.train.drop(self.target, axis=1)\n\n        self.num_features = self.X.select_dtypes(exclude=['object', 'bool']).columns.tolist()\n        self.cat_features = self.X.select_dtypes(include=['object', 'bool']).columns.tolist()\n        to_cat = ['family_history_diabetes', 'hypertension_history', 'cardiovascular_history']\n        self.cat_features = self.cat_features + to_cat\n        self.num_features = [col for col in self.num_features if col not in to_cat]\n\n    def feature_engineering(self, data):\n        df = data.copy()\n        \n        global_stats = {'mean': self.orig[self.target].mean(), 'count': 0}\n        for c in self.num_features + self.cat_features:\n            for a in ['mean', 'count']:\n                col = f'{c}_org_{a}'\n                tmp = (self.orig.groupby(c)[self.target]\n                       .agg(a)\n                       .rename(col)\n                       .reset_index())\n                df = df.merge(tmp, on=c, how='left')\n                df[col] = df[col].fillna(global_stats[a])\n\n        for c in self.cat_features:\n            freqs = df[c].value_counts(normalize=True)\n            df[f\"{c}_fe\"] = df[c].map(freqs)\n\n        df[self.cat_features] = df[self.cat_features].astype('category')\n        return df\n\n    def log_transformation(self):\n        self.y = np.log1p(self.y)\n\n    def remove_outliers(self):\n        Q1 = self.y.quantile(0.25)\n        Q3 = self.y.quantile(0.75)\n        IQR = Q3 - Q1\n        lower_limit = Q1 - 1.5 * IQR\n        upper_limit = Q3 + 1.5 * IQR\n        mask = (self.y >= lower_limit) & (self.y <= upper_limit)\n        self.X = self.X[mask]\n        self.y = self.y[mask]\n        self.X.reset_index(drop=True, inplace=True)\n\n    def missing_values(self):\n        self.X[self.cat_features] = self.X[self.cat_features].fillna('NaN')\n        self.test[self.cat_features] = self.test[self.cat_features].fillna('NaN')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model Training ---\n\nclass Trainer(Config):\n    \n    def __init__(self, X, y, test, models, num_features, cat_features, training=True):\n        self.X = X\n        self.test = test\n        self.y = y\n        self.models = models\n        self.training = training\n        self.scores = pd.DataFrame(columns=['Score'], dtype=float)\n        self.OOF_preds = pd.DataFrame(dtype=float)\n        self.TEST_preds = pd.DataFrame(dtype=float)\n        self.num_features = num_features\n        self.cat_features = cat_features\n\n    def ScoreMetric(self, y_true, y_pred):\n        if self.metric == 'roc_auc':\n            return roc_auc_score(y_true, y_pred, multi_class=\"ovr\") if self.n_classes > 2 else roc_auc_score(y_true, y_pred)\n        elif self.metric == 'accuracy':\n            return accuracy_score(y_true, y_pred)\n        elif self.metric == 'f1':\n            return f1_score(y_true, y_pred, average='weighted') if self.n_classes > 2 else f1_score(y_true, y_pred)\n        elif self.metric == 'precision':\n            return precision_score(y_true, y_pred, average='weighted') if self.n_classes > 2 else precision_score(y_true, y_pred)\n        elif self.metric == 'recall':\n            return recall_score(y_true, y_pred, average='weighted') if self.n_classes > 2 else recall_score(y_true, y_pred)\n        elif self.metric == 'mae':\n            return mean_absolute_error(y_true, y_pred)\n        elif self.metric == 'r2':\n            return r2_score(y_true, y_pred)\n        elif self.metric == 'rmse':\n            return root_mean_squared_error(y_true, y_pred)\n        elif self.metric == 'rmsle':\n            return root_mean_squared_error(y_true, y_pred)\n        elif self.metric == 'mse':\n            return mean_squared_error(y_true, y_pred, squared=True)\n\n    def train(self, model, X, y, test, model_name):\n        oof_pred = np.zeros(X.shape[0], dtype=float)\n        test_pred = np.zeros(test.shape[0], dtype=float)\n\n        print('='*20)\n        print(model_name)\n        params=model.get_params()\n        w_full = np.array([1]*678260+[16]*21740+[8]*100000)\n        for n_fold, (train_id, valid_id) in enumerate(self.folds.split(X, y)):\n            features = X.columns.to_list()\n\n            X_train = X[features].loc[train_id].copy()\n            y_train = y[train_id]\n            X_val = X[features].iloc[valid_id].copy()\n            y_val = y[valid_id]\n            X_test = test[features].copy()\n            w_trn = w_full[train_id]\n            w_val = w_full[valid_id]\n\n            if model_name != 'Ensemble':\n                TE = TargetEncoder(random_state=42, shuffle=True, cv=5, smooth=15)\n                X_train[self.cat_features] = te.fit_transform(X_train[self.cat_features], y_train).astype('float32')\n                X_val[self.cat_features] = te.transform(X_val[self.cat_features]).astype('float32')\n                X_test[self.cat_features] = te.transform(X_test[self.cat_features]).astype('float32')\n            \n            print(f'Fold {n_fold+1}')\n            \n            if \"LGBM\" in model_name:\n                X_train = lightgbm.Dataset(X_train, label=y_train)\n                val_dataset = lightgbm.Dataset(X_val, label=y_val)\n                model = lightgbm.train(\n                    params=params,\n                    train_set=X_train,\n                    valid_sets=[val_dataset],\n                    num_boost_round=100_000,\n                )\n\n            elif any(model in model_name for model in [\"NN\", \"TabM\"]):\n                model.num_features = X_train.select_dtypes(exclude=['category']).columns.tolist()\n                model.cat_features = X_train.select_dtypes(include=['category']).columns.tolist()\n                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n                \n            elif \"XGB\" in model_name:\n                X_train = DMatrix(X_train, label=y_train, enable_categorical=True, weight=w_trn)\n                X_val   = DMatrix(X_val, label=y_val, enable_categorical=True, weight=w_val)\n                X_test  = DMatrix(X_test, enable_categorical=True)\n                model = xgb.train(\n                    params=params,\n                    dtrain=X_train,\n                    evals=[(X_val, \"valid\")],\n                    num_boost_round=100_000,\n                    early_stopping_rounds=200,\n                    verbose_eval=False\n                )\n\n            elif \"CAT\" in model_name:\n                X_train = Pool(X_train, label=y_train, cat_features=self.cat_features)\n                X_val = Pool(X_val, label=y_val, cat_features=self.cat_features)\n                X_test = Pool(test, cat_features=self.cat_features)\n                model.fit(X_train, eval_set=X_val, verbose=False)\n                \n            elif any(model in model_name for model in [\"HGB\", \"YDF\"]):\n                model.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n                \n            elif \"Ensemble\" in model_name:\n                model = Pipeline([\n                    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n                    (\"ridge\", model)\n                ])\n                model.fit(X_train, y_train)\n                \n            else:\n                encoder = FeatureEncoder(num_features=self.num_features, cat_features=self.cat_features)\n                encoder.fit(X)\n                X_train, X_val, X_test = encoder.transform_fold(X_train, X_val, X_test)\n          \n                model.fit(X_train, y_train)\n\n            if self.task_type == \"regression\" :\n                y_pred_val = model.predict(X_val)           \n                test_pred += model.predict(X_test) / self.n_splits\n            elif self.task_type == \"binary\" :\n                y_pred_val = model.predict_proba(X_val)[:, 1]            \n                test_pred += model.predict_proba(X_test)[:, 1] / self.n_splits\n            elif self.task_type == \"multiclass\" :\n                y_pred_val = model.predict_proba(X_val)            \n                test_pred += model.predict_proba(X_test) / self.n_splits\n                \n            oof_pred[valid_id] = y_pred_val\n            score = self.ScoreMetric(y_val, y_pred_val)\n            print(score)\n            self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = score\n\n        self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}'][1:].mean()\n\n        return oof_pred, test_pred\n\n    def run(self):\n        for model_name, model in tqdm(self.models.items()):\n\n            if self.training:                \n                X = self.X.copy()\n                test = self.test.copy()\n\n                oof_pred, test_pred = self.train(model, X, self.y, test, model_name)\n                pd.DataFrame(oof_pred, columns=[f'{model_name}']).to_csv(f'{model_name}_oof.csv', index=False)\n                pd.DataFrame(test_pred, columns=[f'{model_name}']).to_csv(f'{model_name}_test.csv', index=False)\n            \n            else:\n                oof_pred = pd.read_csv(f'/kaggle/input/diabet-models/{model_name}_oof.csv')\n                test_pred = pd.read_csv(f'/kaggle/input/diabet-models/{model_name}_test.csv')\n\n                for n_fold, (train_id, valid_id) in enumerate(self.folds.split(oof_pred, self.y)):\n                    y_pred_val, y_val = oof_pred.loc[valid_id], self.y.loc[valid_id]\n                    self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = self.ScoreMetric(y_val, y_pred_val)\n                self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}'][1:].mean()\n\n            self.OOF_preds[f'{model_name}'] = oof_pred\n            self.TEST_preds[f'{model_name}'] = test_pred\n            \n        if len(self.models)>1:\n            if self.task_is_regression:\n                meta_model = LinearRegression()\n            else:\n                meta_model = LogisticRegression()\n            \n            self.OOF_preds[\"Ensemble\"], self.TEST_preds[\"Ensemble\"] = self.train(meta_model, self.OOF_preds, y, self.TEST_preds, 'Ensemble')            \n            self.scores = self.scores.sort_values('Score')\n            self.score_bar()\n            self.plot_result(self.OOF_preds[\"Ensemble\"])\n            return self.TEST_preds[\"Ensemble\"]\n        else:\n            print(Style.BRIGHT+Fore.GREEN+f'{model_name} score {self.scores.loc[f\"{model_name}\", \"Score\"]:.7f}\\n')\n            self.plot_result(self.OOF_preds[f'{model_name}'])\n            return self.TEST_preds[f'{model_name}']\n            \n    def score_bar(self):\n        plt.figure(figsize=(18, 7))      \n        colors = ['#3cb371' if i != 'Ensemble' else 'r' for i in self.scores.Score.index]\n        hbars = plt.barh(self.scores.index, self.scores.Score, color=colors, height=0.8)\n        plt.bar_label(hbars, fmt='%.6f')\n        plt.ylabel('Models')\n        plt.xlabel('Score')\n        plt.show()\n        \n    def plot_result(self, oof):\n        if self.task_is_regression:\n            cmap = LinearSegmentedColormap.from_list(\"red2green\", [\"#3cb371\", \"r\"], N=10)\n            fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n            \n            errors = np.abs(y - oof)\n            axes[0].scatter(y, oof, c=errors, cmap=cmap, alpha=0.5, s=5)\n            axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n            axes[0].set_xlabel('Actual')\n            axes[0].set_ylabel('Predicted')\n            axes[0].set_title('Actual vs. Predicted')\n            \n            residuals = y - oof\n            axes[1].scatter(oof, residuals, c=errors, cmap=cmap, alpha=0.5, s=5)\n            axes[1].axhline(y=0, color='black', linestyle='--', lw=2)\n            axes[1].set_xlabel('Predicted Values')\n            axes[1].set_ylabel('Residuals')\n            axes[1].set_title('Residual Plot')\n            \n            plt.tight_layout()\n            plt.show()\n        else:\n            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n    \n            for col in self.OOF_preds:\n                RocCurveDisplay.from_predictions(self.y, self.OOF_preds[col], name=f\"{col}\", ax=axes[0])            \n            axes[0].plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n            axes[0].set_xlabel('False Positive Rate')\n            axes[0].set_ylabel('True Positive Rate')\n            axes[0].set_title('ROC')\n            axes[0].legend(loc=\"lower right\")\n            \n            ConfusionMatrixDisplay.from_predictions(y, (oof>=0.5).astype(int), display_labels=self.labels, colorbar=False, ax=axes[1], cmap = 'Greens')\n            axes[1].set_title('Confusion Matrix')\n            \n            plt.tight_layout()\n            plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}